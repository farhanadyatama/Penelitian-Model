{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4WHzOKyxB6s",
    "outputId": "1f5f3a24-19e9-4873-a3e6-2b1a2995ddd8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGWZbNa_Dlju",
    "outputId": "3f20d8e4-7ffb-4c21-c281-374d96a7f6b3"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ezaaputra/indonlu.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTQtx8a2xt9F"
   },
   "source": [
    "# 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ic93GOBxsmL",
    "outputId": "4b501580-ae8d-4a1b-9d51-547927697b33"
   },
   "outputs": [],
   "source": [
    "!pip install emoji\n",
    "!pip install Sastrawi\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaLFp_Fdx1Z7",
    "outputId": "a5621c9d-7158-47c3-9880-0931de1c0700"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import emoji\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Model IndoBERT\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from indonlu.utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader\n",
    "from indonlu.utils.forward_fn import forward_sequence_classification\n",
    "from indonlu.utils.metrics import document_sentiment_metrics_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvkRpYkhzqFq"
   },
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5S3v1f3_zteW"
   },
   "outputs": [],
   "source": [
    "df_path = 'drive/MyDrive/Data/dataset.tsv'\n",
    "stopword_path = 'drive/MyDrive/Data/stopword.txt'\n",
    "kamus_alay_path = 'drive/MyDrive/Data/kamus_alay.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34TH4ndm4RMc"
   },
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(df_path, sep='\\t', names=['text', 'category'])\n",
    "# df = pd.read_csv(df_path, sep=',', names=['text', 'category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "ga3sJtBY4auJ",
    "outputId": "1ed7d08a-208b-4778-ad14-96450c6d1c5d"
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljlNwFGT4d_F",
    "outputId": "f23ca144-b1f0-49ca-b004-9779cd694860"
   },
   "outputs": [],
   "source": [
    "print(f'shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0sfkzl749Lp"
   },
   "source": [
    "## Proporsi Variabel Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "qPGcM7Pu5A0o",
    "outputId": "563bb476-0fc3-4b23-ba62-4a2845bf7faf"
   },
   "outputs": [],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJJz1tDu5F83"
   },
   "outputs": [],
   "source": [
    "# Function to make a donut chart\n",
    "def donut(sizes, ax, angle=90, labels=None, colors=None, explode=None, shadow=None):\n",
    "\n",
    "  # plot\n",
    "  ax.pie(sizes, colors = colors, labels=labels, autopct='%.1f%%',\n",
    "         startangle = angle, pctdistance=0.8, explode = explode,\n",
    "         wedgeprops=dict(width=0.4), shadow=shadow)\n",
    "\n",
    "  # Formatting\n",
    "  plt.axis('equal')\n",
    "  plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nqnWis75qNc"
   },
   "outputs": [],
   "source": [
    "# Plot arguments\n",
    "sizes = df.category.value_counts()\n",
    "labels = ['Positif', 'Negatif', 'Netral']\n",
    "colors = ['lightgreen', 'lightskyblue', 'lightcoral']\n",
    "explode = (0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 436
    },
    "id": "PhGCcNSa6F1t",
    "outputId": "6b7e2ad1-a70e-4633-c118-a0aab0289bc7"
   },
   "outputs": [],
   "source": [
    "# Create axes\n",
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "\n",
    "# plot donut\n",
    "donut(sizes, ax, 90, labels, colors=colors, explode=explode, shadow=True)\n",
    "ax.set_title('Review Category Proportions')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AKb_juN6pCh"
   },
   "source": [
    "# 3. PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUaoUVT85UyT"
   },
   "source": [
    "## Preprocessing Tahap 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGgYO8Th5e94"
   },
   "outputs": [],
   "source": [
    "character = ['.',',',';',':','-,','...','?','!','(',')','[',']','{','}','<','>','\"','/','\\'','#','-','@',\n",
    "             'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
    "             'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z']\n",
    "\n",
    "# hapus karakter yang berulang\n",
    "def repeatcharClean(text):\n",
    "  for i in range(len(character)):\n",
    "    charac_long = 5\n",
    "    while charac_long > 2:\n",
    "      char = character[i]*charac_long\n",
    "      text = text.replace(char, character[i])\n",
    "      charac_long -= 1\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "02_rfB_k583E"
   },
   "outputs": [],
   "source": [
    "def clean_review(text):\n",
    "  # ubah text menjadi huruf kecil\n",
    "  text = text.lower()\n",
    "  # ubah enter menjadi spasi\n",
    "  text = re.sub(r'\\n', ' ', text)\n",
    "  # hapus emoji\n",
    "  text = emoji.demojize(text)\n",
    "  text = re.sub(':[A-Za-z_-]+:', ' ', text) # delete emoji\n",
    "  # hapus emoticon\n",
    "  text = re.sub(r\"([xX;:]'?[dDpPvVoO3)(])\", ' ', text)\n",
    "  # hapus link\n",
    "  text = re.sub(r\"(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\", \"\", text)\n",
    "  # hapus usename\n",
    "  text = re.sub(r\"@[^\\s]+[\\s]?\", ' ', text)\n",
    "  # hapus hashtag\n",
    "  text = re.sub(r'#(\\S+)', r'\\1', text)\n",
    "  # hapus angka dan beberapa simbol\n",
    "  text = re.sub('[^a-zA-Z,.?!]+',' ',text)\n",
    "  # hapus karakter berulang\n",
    "  text = repeatcharClean(text)\n",
    "  # clear spasi\n",
    "  text = re.sub('[ ]+',' ',text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GV83qZZq6aHo"
   },
   "outputs": [],
   "source": [
    "def preprocess_v1(df):\n",
    "  df_pp = df.copy()\n",
    "  df_pp.text = df_pp.text.apply(clean_review)\n",
    "\n",
    "  # delete empty row\n",
    "  # df_pp.text.replace('', np.nan, inplace=True)\n",
    "  # df_pp.text.replace(' ', np.nan, inplace=True)\n",
    "  df_pp['text'] = df_pp['text'].replace(['', ' '], np.nan)\n",
    "\n",
    "  # df_pp.dropna(subset=['text'], inplace=True)\n",
    "  df_pp = df_pp.dropna(subset=['text'])\n",
    "  return df_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_1G2Im8624U"
   },
   "outputs": [],
   "source": [
    "df_v1 = preprocess_v1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VY0UQGzD7Ovh"
   },
   "outputs": [],
   "source": [
    "# export to csv\n",
    "df_v1.to_csv('df_v1.tsv', sep='\\t', header=None, index=False)\n",
    "# df_v1.to_csv('df_v1.csv', sep=',', header=None, encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_nXyBAU7P9z"
   },
   "source": [
    "## Preprocessing Tahap 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gToXpBcu7SIc"
   },
   "outputs": [],
   "source": [
    "# additional_stopword:list kata tdk penting\n",
    "additional_stop = [] #['ya', 'deh', 'dia']\n",
    "\n",
    "# default stopword\n",
    "default_stop = pd.read_csv(stopword_path, sep=',', names=['stopwords'])\n",
    "default_stop = default_stop.stopwords.to_list()\n",
    "\n",
    "# combine all stopword\n",
    "list_stopwords = []\n",
    "list_stopwords = default_stop + additional_stop\n",
    "\n",
    "# remove specific stopword\n",
    "# list_stopwords.remove(\"sangat\")\n",
    "# list_stopwords.remove(\"sekali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nskYY24U7euc"
   },
   "outputs": [],
   "source": [
    "kamus_alay = pd.read_csv(kamus_alay_path)\n",
    "\n",
    "# normalize_word_dict = {}\n",
    "# for index, row in kamus_alay.iterrows():\n",
    "#     if row[0] not in normalize_word_dict:\n",
    "#         normalize_word_dict[row[0]] = row[1]\n",
    "\n",
    "normalize_word_dict = {}\n",
    "for index, row in kamus_alay.iterrows():\n",
    "    if row.iloc[0] not in normalize_word_dict:  # Gunakan .iloc untuk mengakses posisi\n",
    "        normalize_word_dict[row.iloc[0]] = row.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq2rfKDG7f8n"
   },
   "outputs": [],
   "source": [
    "def normalize_review(text):\n",
    "  # tokenize\n",
    "  list_text = word_tokenize(text)\n",
    "\n",
    "  # ubah bahasa alay\n",
    "  list_text = [normalize_word_dict[term] if term in normalize_word_dict else term for term in list_text]\n",
    "\n",
    "  # stemming\n",
    "  factory = StemmerFactory()\n",
    "  stemmer = factory.create_stemmer()\n",
    "  list_text = [stemmer.stem(word) for word in list_text]\n",
    "\n",
    "  # hapus kata yang termasuk stopword\n",
    "  list_text = [word for word in list_text if word not in list_stopwords]\n",
    "\n",
    "  # gabung kembali kalimat\n",
    "  text = \" \".join(list_text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jR67fDFH7it6"
   },
   "outputs": [],
   "source": [
    "def preprocess_v2(df):\n",
    "  df_pp = df.copy()\n",
    "  df_pp.text = df_pp.text.map(normalize_review)\n",
    "\n",
    "  # delete empty row\n",
    "  # df_pp.text.replace('', np.nan, inplace=True)\n",
    "  # df_pp.text.replace(' ', np.nan, inplace=True)\n",
    "  df_pp['text'] = df_pp['text'].replace(['', ' '], np.nan)\n",
    "\n",
    "  # df_pp.dropna(subset=['text'], inplace=True)\n",
    "  df_pp = df_pp.dropna(subset=['text'])\n",
    "  return df_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0_qWj-o7jSA"
   },
   "outputs": [],
   "source": [
    "# take a while\n",
    "df_v2  = preprocess_v2(df_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Lj_UtQU7kQ9"
   },
   "outputs": [],
   "source": [
    "# export to csv\n",
    "df_v2.to_csv('df_v2.tsv', sep='\\t', header=None, index=False)\n",
    "# df_v2.to_csv('df_v2.csv', sep=',', header=None, encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQqF05EI7lVY"
   },
   "outputs": [],
   "source": [
    "def make_corpus(column):\n",
    "\n",
    "    corpus_list = []\n",
    "\n",
    "    for text in column:\n",
    "        cleaned_list = text.split(' ')\n",
    "        corpus_list.extend(cleaned_list)\n",
    "\n",
    "    # transform list of words into 1 body of text\n",
    "    corpus = ' '.join(corpus_list)\n",
    "    corpus = re.sub('[ ]+',' ',corpus) # replace double whitespace with one\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVyLmVji7mXe"
   },
   "outputs": [],
   "source": [
    "# Takes a while\n",
    "corpus = make_corpus(df_v2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCfaSi7p7n8J",
    "outputId": "b1e92e01-bbf8-471f-ec6b-9b848e56efe4"
   },
   "outputs": [],
   "source": [
    "# counting unique words\n",
    "corpus_set = set(corpus.split(' '))\n",
    "\n",
    "print(f'Count of unique words in corpus: {len(corpus_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ON9gOdm47o4n"
   },
   "outputs": [],
   "source": [
    "# function for freqdist\n",
    "def word_freq(corpus, top=5):\n",
    "    tokenized_word = word_tokenize(corpus)\n",
    "    freqdist = FreqDist(tokenized_word)\n",
    "    freqdist = freqdist.most_common(top) # list of tuples\n",
    "\n",
    "    # decompose into label and frequency\n",
    "    label = [tup[0] for tup in freqdist]\n",
    "    freq = [tup[1] for tup in freqdist]\n",
    "    df = pd.DataFrame({'word':label, 'freq':freq})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLtoFy6x7rye"
   },
   "outputs": [],
   "source": [
    "corpus_freq = word_freq(corpus, top=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "lxDZGklc7tWO",
    "outputId": "833ba592-13c3-49e7-c938-28b99ec2d364"
   },
   "outputs": [],
   "source": [
    "# Config params\n",
    "plt.style.use('default')\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "mpl.rcParams['axes.titlesize'] = 20\n",
    "mpl.rcParams['axes.titlepad'] = 20\n",
    "\n",
    "# Compare plots of train and test corpus\n",
    "f, ax1 = plt.subplots(1,figsize=(15,5))\n",
    "\n",
    "sns.barplot(x='word', y='freq', data=corpus_freq, ax=ax1)\n",
    "ax1.set_title('Word Frequency in Train Data')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# sns.barplot(x='word', y='freq', data=corpus_freq, ax=ax2)\n",
    "# ax2.set_title('Word Frequency in Train Data')\n",
    "# ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# sns.despine(ax=ax1)\n",
    "# sns.despine(ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "WuBCFgNe7ukm",
    "outputId": "69530fab-57b3-4888-e2ca-65b671259e1f"
   },
   "outputs": [],
   "source": [
    "# Cloud for corpus\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(corpus)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v7UvP6271RR"
   },
   "source": [
    "# 4. Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1wLOB_T73Re"
   },
   "outputs": [],
   "source": [
    "# train val split\n",
    "# train_set, val_set = train_test_split(df_v2, test_size=0.3, stratify=df_v2.category, random_state=1)\n",
    "# val_set, test_set = train_test_split(val_set, test_size=0.33, stratify=val_set.category, random_state=1)\n",
    "\n",
    "train_set, val_set = train_test_split(df_v2, test_size=0.2, stratify=df_v2.category, random_state=1)\n",
    "val_set, test_set = train_test_split(val_set, test_size=0.2, stratify=val_set.category, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMIivp7F74el",
    "outputId": "bb6db45a-800b-4139-ddf2-7a3b2c9f5948"
   },
   "outputs": [],
   "source": [
    "print(f'Train shape: {train_set.shape}')\n",
    "print(f'Val shape: {val_set.shape}')\n",
    "print(f'Test shape: {test_set.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkb4iAO975_R"
   },
   "outputs": [],
   "source": [
    "# export to csv\n",
    "train_set.to_csv('train_set.tsv', sep='\\t', header=None, index=False)\n",
    "val_set.to_csv('val_set.tsv', sep='\\t', header=None, index=False)\n",
    "test_set.to_csv('test_set.tsv', sep='\\t', header=None, index=False)\n",
    "\n",
    "# train_set.to_csv('train_set.csv', sep=',', header=None, encoding='utf8', index=False)\n",
    "# val_set.to_csv('val_set.csv', sep=',', header=None, encoding='utf8', index=False)\n",
    "# test_set.to_csv('test_set.csv', sep=',', header=None, encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNHHeidH8XlC"
   },
   "source": [
    "# 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlOBr6TI8ttP"
   },
   "source": [
    "## a. Finetuning IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gXm5YwI8v6G"
   },
   "outputs": [],
   "source": [
    "###\n",
    "# common functions\n",
    "###\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTGNKXoX8xII"
   },
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(27)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fisL0Poz80R8"
   },
   "source": [
    "## b. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316,
     "referenced_widgets": [
      "a3c0fb45f00b442dbaf7144ff1b413ce",
      "995c01e607fc4f718fbb879059e02df6",
      "7ce1d401075246e6b83c717b6113ced5",
      "0782ff8f5aae4578ab2f5393259327e2",
      "4186ce9910be4c748521658cfb4b288d",
      "9c6bbfbe0ac74f8b881bd914c391df36",
      "c4e2772994cd4eaa98bc1254ee540e9e",
      "d6c4757208ce4bfaafac4559de998110",
      "f0abbf4f1826415d99af7a692ddbd2e8",
      "45632e1f65c34a37b9a831a5c8fcfe2b",
      "0a7420b57aca4986b7b6a8904f009203",
      "1eef50ad53c843e1bf79d4af16584972",
      "07288be10327482c8ed0339f1699c2ae",
      "c39d51a1dad445799ed710fec4ceb635",
      "9c5218e4416745db9994c698558fd74f",
      "5febb3d950c44e9bb2c082a6cf27aec1",
      "d80ba5e16ece4582b987b7ab27c867d2",
      "163f7ad692ce495f8de5440ef8cd0c5b",
      "7c0243d5f5b54bf592f8c2f5511e035c",
      "0ff82ba642f04b33ae677414d14ea8ca",
      "589980134820491eb38cb681f390e756",
      "92a58d67cfee4778b816476c03c0379c",
      "5ea7e7befdb24fc7abe6f5bbddb832d1",
      "1a9540be027342bc9309e38e048ac41a",
      "4b68e683c60d41ecbb4dcfd43b88a271",
      "58c8fa7bb57a4933b5e84c365dce15d8",
      "0ee37dddaa054cb5b75b8d1e272d1c08",
      "5a554cdfd13142f5b673c2d215778e55",
      "5124f26788d44c0fbc84b5c3d9b7dc63",
      "7cf31a963f2548ceaf9d9731e7fc29ef",
      "60a108a2907648e69b82e748d03dca5a",
      "79257b1b50574cd58e94e9e66c632d48",
      "72efbb649e3a4fa2b9d35ec82f855cd0",
      "721193f6146340d4987dccc2814dfd32",
      "f29cc87178de4d6fbdce8b6afc9cb6ec",
      "4a8fd2ad2c6e4933855c36123a484c85",
      "dd2d618e7d484ed9a3daae405e1579b0",
      "967fe88f02f246b7bdff97432115dd07",
      "39c79ee63eb34d5cb7b4a7bd8dc91865",
      "af33e6e9c4eb456b877d29511caa3ea3",
      "013d19434c274af581ab2fd5626a6d80",
      "177f4d31582e47b1bb3ae30a7e83cf65",
      "4df44c52357b4ef6819d1a5bd75a7796",
      "b9af597336b14a9885d2ab6a69c10bdd",
      "39488050b5774c4cb929ba953921bd8c",
      "5311c05bf6e24fcdb225e58a11fa66d2",
      "375c08fc63ce4f1da0f46a5e61289e92",
      "1c0fcd6259a04794afae44f2b81690c0",
      "7c820f91035942cba2dd6c8eedf69725",
      "4e63490c672e4de5982350fa6bf55d56",
      "586b64feb0ab43ada1e5c1f603a4880f",
      "c0d096adc7e14333a6fb67b92157fb4b",
      "3735a49b939a40d0ba2b1bbbac4bbe30",
      "7a6fc0b6b9fb482b93c1357c4c88c463",
      "43cfbce3309d45819c5b109623039fb8"
     ]
    },
    "id": "pToEROvs818W",
    "outputId": "036bbf2c-5c1d-4ab1-8d80-4e91fda87b6d"
   },
   "outputs": [],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wirHHkHl84cN",
    "outputId": "85cdba6c-45fa-44f2-b10d-2359c0d806c2"
   },
   "outputs": [],
   "source": [
    "# Struktur model\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_uipGfH846M",
    "outputId": "92ec1dd7-1d2a-47e9-937e-c361c288d1cb"
   },
   "outputs": [],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KrzXvAF892P"
   },
   "source": [
    "## c. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1xk-oeY8_ye"
   },
   "outputs": [],
   "source": [
    "train_dataset_path = '/content/train_set.tsv'\n",
    "valid_dataset_path = '/content/val_set.tsv'\n",
    "test_dataset_path = '/content/test_set.tsv'\n",
    "\n",
    "# train_dataset_path = '/content/train_set.csv'\n",
    "# valid_dataset_path = '/content/val_set.csv'\n",
    "# test_dataset_path = '/content/test_set.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nehUA9H19BBV",
    "outputId": "9a9051fe-bf9a-4a4e-a162-aadad8f67821"
   },
   "outputs": [],
   "source": [
    "# fungsi dataset loader dari utils IndoNLU\n",
    "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
    "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "# train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=True)\n",
    "# valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)\n",
    "# test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=16, shuffle=False)\n",
    "\n",
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=32, num_workers=4, shuffle=True)\n",
    "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=32, num_workers=4, shuffle=False)\n",
    "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=32, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-60WzGSI9CGe",
    "outputId": "dbb446f4-0e62-499e-a12e-c84349feffe9"
   },
   "outputs": [],
   "source": [
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "print(w2i) #word to index\n",
    "print(i2w) #index to word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIpkjvQ19GHs"
   },
   "source": [
    "## d. Uji coba pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxCQxsHH9IwV",
    "outputId": "e41cede6-d848-473d-c536-dbf3daec39e1"
   },
   "outputs": [],
   "source": [
    "text = 'Pemerintahan Prabowo dan Gibran dianggap gagal total dalam menepati janji kampanye. Banyak program tidak jelas arahnya dan rakyat merasa kecewa dengan hasil kerja yang jauh dari harapan.'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dsWeJKfg9NUf",
    "outputId": "fa1847e8-d400-4b9d-ad64-7bb77446094a"
   },
   "outputs": [],
   "source": [
    "text = 'Di bawah kepemimpinan Prabowo dan Gibran, pemerintah menetapkan sejumlah program strategis untuk pembangunan nasional. apakah sesuai dengan visi dan misinya?'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7o3nsx1P9PoR",
    "outputId": "a1d3f59d-b87d-499a-9341-9bc8a7b039bf"
   },
   "outputs": [],
   "source": [
    "text = 'Pemerintahan Prabowo dan Gibran membawa angin segar bagi Indonesia dengan kebijakan inovatif yang fokus pada kemajuan ekonomi, pendidikan, dan kesejahteraan rakyat.'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rghsUCC39UXM"
   },
   "source": [
    "## e. Fine Tuning & Prediksi Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52Ke8JST9Xhs"
   },
   "outputs": [],
   "source": [
    "# Tentukan optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-6)\n",
    "model = model.cuda()\n",
    "# model = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asHHvTu6aCcX"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746,
     "referenced_widgets": [
      "29eb7a7066424949b8b930702a8ba1d6",
      "3d4361007d6047b0a25e54e508144d4c",
      "295dcb9ad4d54d64a2f2658ba3a781ec",
      "9566ece3b01b4a53804588c43f1098ca",
      "04867956f63b4c72905fe2034e3dd636",
      "da1e6b897efe417fb6f83e28943e6364",
      "347677574c40455ba4deb07e92487ed1",
      "7fd410288f2e4c8f96dd5ec8703ee3d4",
      "b0b5e1950f504723be4a837203dc3f61",
      "1f80aaa82f0e47dcb77c186cb25c8664",
      "0df68ad8247e4948a4eebe9c569baee9"
     ]
    },
    "id": "XjmD9dnHaD0c",
    "outputId": "f63e2513-6772-4af7-fb76-f8e48dc41bd9"
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "n_epochs = 5\n",
    "history = defaultdict(list)\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    total_train_loss = 0\n",
    "    list_hyp_train, list_label = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "    for i, batch_data in enumerate(train_pbar):\n",
    "        # Forward model\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Update model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_loss = loss.item()\n",
    "        total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "        # Hitung skor train metrics\n",
    "        list_hyp_train += batch_hyp\n",
    "        list_label += batch_label\n",
    "\n",
    "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp_train, list_label)\n",
    "    print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "        total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "    # save train acc for learning curve\n",
    "    history['train_acc'].append(metrics['ACC'])\n",
    "\n",
    "    # Evaluate di validation set\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]\n",
    "        loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        # Hitung total loss\n",
    "        valid_loss = loss.item()\n",
    "        total_loss = total_loss + valid_loss\n",
    "\n",
    "        # Hitung skor evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "\n",
    "    metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "    print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "        total_loss/(i+1), metrics_to_string(metrics)))\n",
    "\n",
    "    # save validation acc for learning curve\n",
    "    history['val_acc'].append(metrics['ACC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQ4Axw4DaJHx"
   },
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "sh1Wsk53aNZF",
    "outputId": "f27d27e9-ed58-4c4a-ce4a-18a1dd4f5e85"
   },
   "outputs": [],
   "source": [
    "plt.plot(history['train_acc'], label='train acc')\n",
    "plt.plot(history['val_acc'], label='validation acc')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iMNcYKOaPHr"
   },
   "outputs": [],
   "source": [
    "# Simpan Hasil Prediksi Validation Set\n",
    "val_df = pd.read_csv(valid_dataset_path, sep='\\t', names=['text', 'category'])\n",
    "val_df['pred'] = list_hyp\n",
    "val_df.head()\n",
    "val_df.to_csv('val result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVzl2w5OaT_4"
   },
   "source": [
    "## f. Prediksi Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDPbKZDdacV2",
    "outputId": "121ac82e-14ef-47fa-8b9d-32aa01701692"
   },
   "outputs": [],
   "source": [
    "# Prediksi test set\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "total_loss, total_correct, total_labels = 0, 0, 0\n",
    "pred, list_label = [], []\n",
    "\n",
    "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "for i, batch_data in enumerate(pbar):\n",
    "    _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "    pred += batch_hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m8NPhbDaekR"
   },
   "outputs": [],
   "source": [
    "# Simpan prediksi test set\n",
    "test_df = pd.read_csv(test_dataset_path, sep='\\t', names=['text', 'category'])\n",
    "test_df['pred'] = pred\n",
    "test_df.head()\n",
    "test_df.to_csv('test result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rytXedVTajRY"
   },
   "source": [
    "## g. Test fine-tuned model on sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZOnwH1vAapyR",
    "outputId": "3ea4a244-8b3d-4a4f-dcf8-c5b6d52ae230"
   },
   "outputs": [],
   "source": [
    "text = 'Pemerintahan Prabowo dan Gibran dianggap gagal total dalam menepati janji kampanye. Banyak program tidak jelas arahnya dan rakyat merasa kecewa dengan hasil kerja yang jauh dari harapan.'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jalATxf1atBx",
    "outputId": "512d4674-323f-4b9f-dabe-f825b56747ac"
   },
   "outputs": [],
   "source": [
    "text = 'Pemerintahan Prabowo dan Gibran membawa angin segar bagi Indonesia dengan kebijakan inovatif yang fokus pada kemajuan ekonomi, pendidikan, dan kesejahteraan rakyat.'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "obLyaGk4avBg",
    "outputId": "bc5832a4-b6d0-4e0b-a558-d1c72bc1ecb3"
   },
   "outputs": [],
   "source": [
    "text = 'Di bawah kepemimpinan Prabowo dan Gibran, pemerintah menetapkan sejumlah program strategis untuk pembangunan nasional. apakah sesuai dengan visi dan misinya?'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mABwiNP_axFN"
   },
   "source": [
    "# 6. Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cnn2LjWsa0UL"
   },
   "outputs": [],
   "source": [
    "val_real = val_df.category\n",
    "val_pred = val_df.pred\n",
    "\n",
    "test_real = test_df.category\n",
    "test_pred = test_df.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "D1bohVh6a4Pr",
    "outputId": "8cc2ac13-ff39-4145-a102-5c52342fe592"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(val_real, val_pred)\n",
    "df_cm = pd.DataFrame(cm, index=['positive', 'neutral', 'negative'], columns=['positive', 'neutral', 'negative'])\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2aWUIO4Ia8i9",
    "outputId": "6e99d814-bdc8-47c8-c10b-4dfe5f0b560b"
   },
   "outputs": [],
   "source": [
    "print(classification_report(val_real, val_pred, target_names=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "O_BFAkmha9yy",
    "outputId": "29a9b5fc-2ba0-4546-a3ca-bd2b14ef9d23"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment');\n",
    "\n",
    "cm = confusion_matrix(test_real, test_pred)\n",
    "df_cm = pd.DataFrame(cm, index=['positive', 'neutral', 'negative'], columns=['positive', 'neutral', 'negative'])\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3pjVzv6a_ig",
    "outputId": "6f2e8cb4-ba04-497a-c6b5-18bee207ea7c"
   },
   "outputs": [],
   "source": [
    "print(classification_report(test_real, test_pred, target_names=['positive', 'neutral', 'negative']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4tJ4Xnk02yJ"
   },
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nP1YGrSN000_"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Misal: setelah fine-tuning IndoBERT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "# Simpan model dan tokenizer\n",
    "model.save_pretrained(\"indobert/\")\n",
    "tokenizer.save_pretrained(\"indobert/\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"saved_indobert/pytorch_model.bin\")\n",
    "# model.config.to_json_file(\"saved_indobert/config.json\")\n",
    "# tokenizer.save_pretrained(\"saved_indobert/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kKM3fIw06sl"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"indobert/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4WBVHAI087n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Isi folder test_bert:\", os.listdir(\"indobert/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KddDOIE209Sx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(\"indobert/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIjAW-AAO1XE"
   },
   "source": [
    "# Prediksi Sentimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YeSMz-BtO4Qc",
    "outputId": "68272b9c-3fab-4196-a5af-dc69fbc8b618"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- SETUP PATH DAN DEVICE ---\n",
    "model_path = \"drive/MyDrive/indobert\"  # Ganti dengan path folder model kamu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- LOAD MODEL & TOKENIZER ---\n",
    "# This will load config.json (with num_labels=3) and pytorch_model.bin\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- LABEL MAP (PASTIKAN SESUAI TRAINING) ---\n",
    "# Map the model's output indices (0, 1, 2) to your sentiment labels (0, 1, 2)\n",
    "# Based on the w2i output earlier: {'neutral': 0, 'positive': 1, 'negative': 2}\n",
    "# and the order in the confusion matrix (positive, neutral, negative),\n",
    "# it seems your labels in the DataFrame are 'Positif', 'Netral', 'Negatif'.\n",
    "# Let's confirm the mapping from the model's output index to your desired label integer.\n",
    "# Based on your training data and the w2i map {0: 'neutral', 1: 'positive', 2: 'negative'},\n",
    "# the model outputs 0 for neutral, 1 for positive, and 2 for negative.\n",
    "# Your label_map seems to map model output index to itself, which is correct if\n",
    "# your desired output integers are 0 for neutral, 1 for positive, 2 for negative.\n",
    "# If your DataFrame labels are strings, you'll need to map the predicted index back to string.\n",
    "# Assuming 0->Netral, 1->Positif, 2->Negatif for the final output:\n",
    "label_map_to_string = {\n",
    "    0: 'Positive',\n",
    "    1: 'Neutral',\n",
    "    2: 'Negative'\n",
    "}\n",
    "\n",
    "\n",
    "# --- FUNGSI PREDIKSI LABEL ---\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = F.softmax(outputs.logits, dim=1)\n",
    "    pred_index = torch.argmax(probs, dim=1).item()\n",
    "    # Map the predicted index to the desired string label\n",
    "    return label_map_to_string.get(pred_index, 'Unknown') # Return 'Unknown' if index is unexpected\n",
    "\n",
    "\n",
    "# --- LOAD FILE EXCEL ---\n",
    "df = pd.read_csv(\"drive/MyDrive/Data/data_baru_prediksi.csv\")  # Ganti nama file Excel kamu\n",
    "\n",
    "# --- LAKUKAN PREDIKSI & TAMBAHKAN KOLOM SENTIMEN ---\n",
    "# Add error handling in case any text causes issues during prediction\n",
    "def safe_predict_sentiment(text):\n",
    "    if pd.isna(text): # Handle NaN or None input\n",
    "        return None\n",
    "    try:\n",
    "        return predict_sentiment(str(text)) # Ensure input is string\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting sentiment for text: {str(text)[:50]}... Error: {e}\")\n",
    "        return None # Or some other indicator of failure\n",
    "\n",
    "df[\"sentiment\"] = df[\"text\"].apply(safe_predict_sentiment)\n",
    "\n",
    "# --- SIMPAN KE FILE BARU ---\n",
    "df.to_csv(\"hasil_sentimen_label.csv\", index=False)\n",
    "\n",
    "print(\"✅ Pelabelan selesai! File disimpan sebagai 'hasil_sentimen_label.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4yvHq4oO7Qg",
    "outputId": "f45ba21d-e4f7-4a66-e1c4-50ac37693c36"
   },
   "outputs": [],
   "source": [
    "print(df[\"sentiment\"].value_counts())\n",
    "print(df[\"sentiment\"].unique())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wTQtx8a2xt9F",
    "UvkRpYkhzqFq",
    "6AKb_juN6pCh",
    "E_nXyBAU7P9z",
    "4v7UvP6271RR",
    "ZNHHeidH8XlC",
    "rghsUCC39UXM",
    "rVzl2w5OaT_4",
    "rytXedVTajRY",
    "mABwiNP_axFN"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
