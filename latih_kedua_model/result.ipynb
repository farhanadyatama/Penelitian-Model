{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFBRm9mI6UnF",
    "outputId": "fdc279fc-959d-497b-91a8-fb89f2959b45"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccH453gW49J4",
    "outputId": "b28c308c-b96d-42d9-c4ad-5345a42aeb44"
   },
   "outputs": [],
   "source": [
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czr1eG0V6o8y",
    "outputId": "4e27675f-6db7-4e9f-b35e-dd34a532b9d2"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5C-ZubehmUO"
   },
   "source": [
    "# IndoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "Kx2Qcjfc51TZ",
    "outputId": "c95f288f-a326-462d-afb1-effc09a34fea"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "import torch\n",
    "\n",
    "# âœ… Nonaktifkan Weights & Biases agar tidak minta API key\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# ðŸ”§ Konfigurasi\n",
    "model_path = \"drive/MyDrive/indobert\"  # Folder berisi config.json & pytorch_model.bin\n",
    "max_seq_len = 512\n",
    "batch_size = 32\n",
    "lr = 3e-6\n",
    "epochs = 5\n",
    "random_state = 1\n",
    "test_size = 0.2\n",
    "\n",
    "# âœ… Mapping label teks â†’ angka\n",
    "label2id = {\n",
    "    \"positive\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"negative\": 2\n",
    "}\n",
    "\n",
    "# âœ… Load model dan tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "\n",
    "# âœ… Load dan map dataset\n",
    "csv_path = \"drive/MyDrive/Data/dataset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df[\"sentiment\"].isin(label2id.keys())]  # filter hanya label valid\n",
    "df[\"label\"] = df[\"sentiment\"].map(label2id)\n",
    "\n",
    "# âœ… Split data\n",
    "train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# âœ… Tokenisasi dan tambahkan ke DataFrame\n",
    "def tokenize_and_add_to_df(dataframe):\n",
    "    texts = dataframe[\"text\"].tolist()\n",
    "    labels = dataframe[\"label\"].tolist()\n",
    "    encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_seq_len)\n",
    "    dataframe[\"input_ids\"] = encodings[\"input_ids\"]\n",
    "    dataframe[\"token_type_ids\"] = encodings.get(\"token_type_ids\", [[0]*max_seq_len]*len(texts))\n",
    "    dataframe[\"attention_mask\"] = encodings[\"attention_mask\"]\n",
    "    return dataframe\n",
    "\n",
    "train_df = tokenize_and_add_to_df(train_df)\n",
    "test_df = tokenize_and_add_to_df(test_df)\n",
    "\n",
    "# âœ… Dataset HuggingFace\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Pilih hanya kolom yang diperlukan\n",
    "selected_cols = [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"]\n",
    "train_dataset = train_dataset.select_columns(selected_cols)\n",
    "test_dataset = test_dataset.select_columns(selected_cols)\n",
    "\n",
    "# âœ… Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"results/\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs/\"\n",
    ")\n",
    "\n",
    "# âœ… Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "# âœ… Fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7aoQ1xd3LPx",
    "outputId": "7d3d597a-8729-4cde-b804-b0a010daf396"
   },
   "outputs": [],
   "source": [
    "# Tentukan path di Google Drive Anda (Anda bisa membuat folder baru)\n",
    "drive_path = \"/content/drive/MyDrive/model_indobert/\"\n",
    "\n",
    "# Simpan model dan tokenizer ke path tersebut\n",
    "trainer.save_model(drive_path)\n",
    "tokenizer.save_pretrained(drive_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhcTPoGchsqo"
   },
   "source": [
    "# XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "lPPOm25jh3QW",
    "outputId": "d786d976-1ae1-47e2-d5bc-2d2cef5991b7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    XLNetForSequenceClassification,\n",
    "    XLNetTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "import torch\n",
    "\n",
    "# âœ… Nonaktifkan Weights & Biases\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# ðŸ”§ Konfigurasi\n",
    "model_path = \"drive/MyDrive/xlnet\"  # folder harus berisi config.json + spiece.model + pytorch_model.bin\n",
    "max_seq_len = 512\n",
    "batch_size = 4\n",
    "lr = 3e-6\n",
    "epochs = 5\n",
    "random_state = 1\n",
    "test_size = 0.4\n",
    "\n",
    "# âœ… Mapping label\n",
    "label2id = {\n",
    "    \"positive\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"negative\": 2\n",
    "}\n",
    "\n",
    "# âœ… Load model dan tokenizer\n",
    "model = XLNetForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=3,\n",
    "    id2label={str(v): k for k, v in label2id.items()},\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True # Add this line to ignore size mismatches\n",
    ")\n",
    "\n",
    "tokenizer = XLNetTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# âœ… Load dan siapkan data\n",
    "csv_path = \"drive/MyDrive/Data/dataset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[df[\"sentiment\"].isin(label2id.keys())]\n",
    "df[\"label\"] = df[\"sentiment\"].map(label2id)\n",
    "\n",
    "# âœ… Split data\n",
    "train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# âœ… Tokenisasi\n",
    "def tokenize_and_add_to_df(dataframe):\n",
    "    texts = dataframe[\"text\"].tolist()\n",
    "    labels = dataframe[\"label\"].tolist()\n",
    "    encodings = tokenizer(texts, truncation=True, padding=\"max_length\", max_length=max_seq_len)\n",
    "    dataframe[\"input_ids\"] = encodings[\"input_ids\"]\n",
    "    dataframe[\"token_type_ids\"] = encodings.get(\"token_type_ids\", [[0]*max_seq_len]*len(texts))\n",
    "    dataframe[\"attention_mask\"] = encodings[\"attention_mask\"]\n",
    "    return dataframe\n",
    "\n",
    "train_df = tokenize_and_add_to_df(train_df)\n",
    "test_df = tokenize_and_add_to_df(test_df)\n",
    "\n",
    "# âœ… Buat HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Pilih kolom input\n",
    "selected_cols = [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"label\"]\n",
    "train_dataset = train_dataset.select_columns(selected_cols)\n",
    "test_dataset = test_dataset.select_columns(selected_cols)\n",
    "\n",
    "# âœ… Argumen training\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"none\",\n",
    "    output_dir=\"results_xlnet/\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"logs_xlnet/\"\n",
    ")\n",
    "\n",
    "# âœ… Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=default_data_collator\n",
    ")\n",
    "\n",
    "# âœ… Mulai training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "-oxBtr1hhvTV",
    "outputId": "94cd568a-48ac-44fc-80fa-c306681301ab"
   },
   "outputs": [],
   "source": [
    "# Tentukan path di Google Drive Anda (Anda bisa membuat folder baru)\n",
    "drive_path = \"/content/drive/MyDrive/model_xlnet/\"\n",
    "\n",
    "# Simpan model dan tokenizer ke path tersebut\n",
    "trainer.save_model(drive_path)\n",
    "tokenizer.save_pretrained(drive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xs900_dgnJr3",
    "outputId": "ad3fa657-d095-4597-c77e-cf928e693ed4"
   },
   "outputs": [],
   "source": [
    "print(model.logits_proj.weight.shape)  # Harusnya (3, 768)\n",
    "print(model.config.num_labels)        # Harusnya 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaqKsTFo7kht"
   },
   "source": [
    "# Sentiment (Excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BiB2mLze7Aa_",
    "outputId": "489fbb35-7c27-4bc9-bafb-ce669bf12d09"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- SETUP PATH DAN DEVICE ---\n",
    "model_path = \"drive/MyDrive/model_indobert\"  # Ganti dengan path folder model kamu\n",
    "# model_path = \"drive/MyDrive/model_xlnet\"  # Ganti dengan path folder model kamu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- LOAD MODEL & TOKENIZER ---\n",
    "# This will load config.json (with num_labels=3) and pytorch_model.bin\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# --- LABEL MAP (PASTIKAN SESUAI TRAINING) ---\n",
    "# Map the model's output indices (0, 1, 2) to your sentiment labels (0, 1, 2)\n",
    "# Based on the w2i output earlier: {'neutral': 0, 'positive': 1, 'negative': 2}\n",
    "# and the order in the confusion matrix (positive, neutral, negative),\n",
    "# it seems your labels in the DataFrame are 'Positif', 'Netral', 'Negatif'.\n",
    "# Let's confirm the mapping from the model's output index to your desired label integer.\n",
    "# Based on your training data and the w2i map {0: 'neutral', 1: 'positive', 2: 'negative'},\n",
    "# the model outputs 0 for neutral, 1 for positive, and 2 for negative.\n",
    "# Your label_map seems to map model output index to itself, which is correct if\n",
    "# your desired output integers are 0 for neutral, 1 for positive, 2 for negative.\n",
    "# If your DataFrame labels are strings, you'll need to map the predicted index back to string.\n",
    "# Assuming 0->Netral, 1->Positif, 2->Negatif for the final output:\n",
    "label_map_to_string = {\n",
    "    0: 'Positive',\n",
    "    1: 'Neutral',\n",
    "    2: 'Negative'\n",
    "}\n",
    "\n",
    "\n",
    "# --- FUNGSI PREDIKSI LABEL ---\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probs = F.softmax(outputs.logits, dim=1)\n",
    "    pred_index = torch.argmax(probs, dim=1).item()\n",
    "    # Map the predicted index to the desired string label\n",
    "    return label_map_to_string.get(pred_index, 'Unknown') # Return 'Unknown' if index is unexpected\n",
    "\n",
    "\n",
    "# --- LOAD FILE EXCEL ---\n",
    "df = pd.read_csv(\"drive/MyDrive/Data/data_baru_prediksi.csv\")  # Ganti nama file Excel kamu\n",
    "\n",
    "# --- LAKUKAN PREDIKSI & TAMBAHKAN KOLOM SENTIMEN ---\n",
    "# Add error handling in case any text causes issues during prediction\n",
    "def safe_predict_sentiment(text):\n",
    "    if pd.isna(text): # Handle NaN or None input\n",
    "        return None\n",
    "    try:\n",
    "        return predict_sentiment(str(text)) # Ensure input is string\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting sentiment for text: {str(text)[:50]}... Error: {e}\")\n",
    "        return None # Or some other indicator of failure\n",
    "\n",
    "df[\"sentiment\"] = df[\"text\"].apply(safe_predict_sentiment)\n",
    "\n",
    "# --- SIMPAN KE FILE BARU ---\n",
    "df.to_csv(\"hasil_sentimen_label.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Pelabelan selesai! File disimpan sebagai 'hasil_sentimen_label.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQlfxcVGqqur",
    "outputId": "a8ba648b-9325-4530-bc3b-b697e718b24f"
   },
   "outputs": [],
   "source": [
    "print(df[\"sentiment\"].value_counts())\n",
    "print(df[\"sentiment\"].unique())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "B5C-ZubehmUO",
    "UhcTPoGchsqo"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
